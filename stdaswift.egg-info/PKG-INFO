Metadata-Version: 2.4
Name: stdaswift
Version: 0.1.0
Summary: STDA-SwiFT: 4D Swin Transformer encoder release for fMRI
Author: brian880825
Project-URL: Homepage, https://github.com/brian880825/STDA-SwiFT
Requires-Python: >=3.8
Description-Content-Type: text/markdown
Requires-Dist: torch>=2.1
Requires-Dist: numpy>=1.23
Requires-Dist: monai>=1.3
Requires-Dist: nibabel>=5.1
Requires-Dist: einops>=0.7

# STDA-SwiFT (Official Repository)

This is the official repository for the paper:

Whole-brain Transferable Representations from Large-Scale fMRI Data Improve Task-Evoked Brain Activity Decoding ([arXiv:2507.22378](https://arxiv.org/abs/2507.22378)).

It provides the STDA-SwiFT pretrained encoder and a minimal inference example for generating embeddings from 4D fMRI volumes.

## Install

```bash
pip install -r requirements.txt
# optional: install package locally for imports
pip install -e .
```

## Quick start

```bash
python examples/infer.py --input /path/to/volume.npy --ckpt ckpt/pretrained.ckpt
```

## Finetune on Haxby classification (example)

```bash
pip install nilearn
python examples/finetune_haxby.py --ckpt ckpt/pretrained.ckpt \
  --subject 1 --seq_len 1 --batch_size 2 --epochs 5 --lr 1e-4 --freeze_encoder
```

- Trains a linear head (or full model if you omit `--freeze_encoder`) on Haxby categories
  using contiguous windows of length `--seq_len` with constant labels and resized to `(96,96,96)`.
- Requires internet to download Haxby via `nilearn` on first run.
- You can increase `--seq_len` to leverage temporal context; the script will interpolate the time
  embeddings to the requested length.

- `ckpt/pretrained.ckpt` is the pretrained task-fMRI encoder.
- The script prints the embedding vector shape and a preview of values.
- Input may be `.npy`, `.pt`, or `.nii/.nii.gz` with 4D (H,W,D,T) fMRI data.
 - `--seq_len` is optional; if omitted, the script infers it from input.
- We recommend padding the input to `(96,96,96,T)` before any further training or inference.

## Checkpoint format

The loader accepts either:
- A plain `state_dict` of the encoder, or
- A Lightning checkpoint with keys prefixed by `encoder.` or `model.`

## Model

`stdaswift.models.SwinTransformer4D` with:
- `img_size=(96,96,96,T)`
- `patch_size=(6,6,6,1)`
- `depths=(2,2,18,2)`
- `num_heads=(3,6,12,24)`
- `window_size=(6,6,6,1)`

## Citation

If you find this work useful, please cite:

Peng, Y.-P., Cheung, V.K.M., Su, L. Whole-brain Transferable Representations from Large-Scale fMRI Data Improve Task-Evoked Brain Activity Decoding. arXiv:2507.22378, 2025. ([link](https://arxiv.org/abs/2507.22378))

BibTeX:

```
@article{Peng2025STDA-SwiFT,
  title={Whole-brain Transferable Representations from Large-Scale fMRI Data Improve Task-Evoked Brain Activity Decoding},
  author={Yueh-Po Peng and Vincent K. M. Cheung and Li Su},
  journal={arXiv preprint arXiv:2507.22378},
  year={2025}
}
```
